# AI助手模块 - 模型集成方案

## 1. 模型集成概述

### 1.1 集成目标
- **多模型支持**: 支持本地和云端多种AI模型
- **灵活切换**: 用户可根据需求选择不同模型
- **成本控制**: 平衡性能和成本，提供最优方案
- **扩展性**: 支持新模型的快速集成

### 1.2 集成架构
```
模型集成层
├── 本地模型
│   ├── Hugging Face Transformers
│   ├── ONNX Runtime
│   └── TensorFlow/PyTorch
├── 云端模型
│   ├── OpenAI API
│   ├── Azure OpenAI
│   ├── 百度文心一言
│   └── 阿里通义千问
├── 模型管理
│   ├── 模型注册
│   ├── 配置管理
│   ├── 性能监控
│   └── 负载均衡
└── 统一接口
    ├── 标准化API
    ├── 参数转换
    ├── 响应处理
    └── 错误处理
```

## 2. 本地模型集成

### 2.1 Hugging Face Transformers

#### 2.1.1 模型选择
```python
# 推荐的本地模型
LOCAL_MODELS = {
    'text_generation': {
        'gpt2': 'gpt2',
        'gpt2-medium': 'gpt2-medium',
        'gpt2-large': 'gpt2-large',
        'chinese-gpt2': 'uer/gpt2-chinese-cluecorpussmall'
    },
    'text_classification': {
        'bert-base-chinese': 'bert-base-chinese',
        'roberta-base': 'roberta-base'
    },
    'question_answering': {
        'bert-base-chinese': 'bert-base-chinese',
        'distilbert-base': 'distilbert-base-cased'
    },
    'text_summarization': {
        'bart-base': 'facebook/bart-base',
        't5-small': 't5-small'
    }
}
```

#### 2.1.2 模型加载器
```python
from transformers import AutoTokenizer, AutoModel, pipeline
import torch
from typing import Dict, Any

class LocalModelLoader:
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.pipelines = {}
        
    def load_model(self, model_name: str, task_type: str) -> Any:
        """加载本地模型"""
        try:
            if task_type == 'text_generation':
                return self._load_text_generation_model(model_name)
            elif task_type == 'text_classification':
                return self._load_classification_model(model_name)
            elif task_type == 'question_answering':
                return self._load_qa_model(model_name)
            elif task_type == 'text_summarization':
                return self._load_summarization_model(model_name)
            else:
                raise ValueError(f"不支持的任务类型: {task_type}")
        except Exception as e:
            logger.error(f"加载模型失败: {e}")
            return None
    
    def _load_text_generation_model(self, model_name: str):
        """加载文本生成模型"""
        if model_name not in self.models:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            
            # 设置pad_token
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            self.models[model_name] = model
            self.tokenizers[model_name] = tokenizer
            
        return {
            'model': self.models[model_name],
            'tokenizer': self.tokenizers[model_name]
        }
    
    def _load_classification_model(self, model_name: str):
        """加载分类模型"""
        if model_name not in self.pipelines:
            self.pipelines[model_name] = pipeline(
                'text-classification',
                model=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
        return self.pipelines[model_name]
    
    def _load_qa_model(self, model_name: str):
        """加载问答模型"""
        if model_name not in self.pipelines:
            self.pipelines[model_name] = pipeline(
                'question-answering',
                model=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
        return self.pipelines[model_name]
    
    def _load_summarization_model(self, model_name: str):
        """加载摘要模型"""
        if model_name not in self.pipelines:
            self.pipelines[model_name] = pipeline(
                'summarization',
                model=model_name,
                device=0 if torch.cuda.is_available() else -1
            )
        return self.pipelines[model_name]
```

#### 2.1.3 模型推理
```python
class LocalModelInference:
    def __init__(self, model_loader: LocalModelLoader):
        self.model_loader = model_loader
        
    def generate_text(self, model_name: str, prompt: str, **kwargs) -> str:
        """文本生成"""
        model_data = self.model_loader.load_model(model_name, 'text_generation')
        if not model_data:
            return "模型加载失败"
        
        model = model_data['model']
        tokenizer = model_data['tokenizer']
        
        # 编码输入
        inputs = tokenizer.encode(prompt, return_tensors='pt')
        
        # 生成参数
        generation_config = {
            'max_length': kwargs.get('max_length', 100),
            'temperature': kwargs.get('temperature', 0.7),
            'top_p': kwargs.get('top_p', 0.9),
            'do_sample': kwargs.get('do_sample', True),
            'pad_token_id': tokenizer.eos_token_id
        }
        
        # 生成文本
        with torch.no_grad():
            outputs = model.generate(inputs, **generation_config)
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return generated_text
    
    def classify_text(self, model_name: str, text: str) -> Dict[str, Any]:
        """文本分类"""
        pipeline = self.model_loader.load_model(model_name, 'text_classification')
        if not pipeline:
            return {"error": "模型加载失败"}
        
        result = pipeline(text)
        return result
    
    def answer_question(self, model_name: str, question: str, context: str) -> Dict[str, Any]:
        """问答"""
        pipeline = self.model_loader.load_model(model_name, 'question_answering')
        if not pipeline:
            return {"error": "模型加载失败"}
        
        result = pipeline(question=question, context=context)
        return result
    
    def summarize_text(self, model_name: str, text: str, **kwargs) -> str:
        """文本摘要"""
        pipeline = self.model_loader.load_model(model_name, 'text_summarization')
        if not pipeline:
            return "模型加载失败"
        
        max_length = kwargs.get('max_length', 130)
        min_length = kwargs.get('min_length', 30)
        
        result = pipeline(text, max_length=max_length, min_length=min_length)
        return result[0]['summary_text']
```

### 2.2 ONNX Runtime集成

#### 2.2.1 模型转换
```python
import onnx
import onnxruntime as ort
from transformers import AutoTokenizer, AutoModel
import torch

class ONNXModelConverter:
    def __init__(self):
        self.onnx_models = {}
        
    def convert_to_onnx(self, model_name: str, task_type: str, output_path: str):
        """转换模型为ONNX格式"""
        try:
            if task_type == 'text_generation':
                return self._convert_generation_model(model_name, output_path)
            elif task_type == 'text_classification':
                return self._convert_classification_model(model_name, output_path)
            else:
                raise ValueError(f"不支持的任务类型: {task_type}")
        except Exception as e:
            logger.error(f"模型转换失败: {e}")
            return False
    
    def _convert_generation_model(self, model_name: str, output_path: str):
        """转换生成模型"""
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        
        # 设置模型为评估模式
        model.eval()
        
        # 创建示例输入
        dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 10))
        
        # 导出ONNX模型
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input_ids'],
            output_names=['output'],
            dynamic_axes={
                'input_ids': {0: 'batch_size', 1: 'sequence'},
                'output': {0: 'batch_size', 1: 'sequence'}
            }
        )
        
        return True
    
    def load_onnx_model(self, model_path: str):
        """加载ONNX模型"""
        if model_path not in self.onnx_models:
            session = ort.InferenceSession(model_path)
            self.onnx_models[model_path] = session
        return self.onnx_models[model_path]
```

#### 2.2.2 ONNX推理
```python
class ONNXInference:
    def __init__(self, converter: ONNXModelConverter):
        self.converter = converter
        
    def generate_with_onnx(self, model_path: str, tokenizer, input_ids, **kwargs):
        """使用ONNX模型生成文本"""
        session = self.converter.load_onnx_model(model_path)
        
        # 准备输入
        ort_inputs = {
            'input_ids': input_ids.numpy()
        }
        
        # 执行推理
        ort_outputs = session.run(None, ort_inputs)
        
        # 处理输出
        outputs = torch.from_numpy(ort_outputs[0])
        
        return outputs
```

## 3. 云端模型集成

### 3.1 OpenAI API集成

#### 3.1.1 API客户端
```python
import openai
import aiohttp
import asyncio
from typing import Dict, Any, List

class OpenAIClient:
    def __init__(self, api_key: str, base_url: str = None):
        self.api_key = api_key
        self.base_url = base_url or "https://api.openai.com/v1"
        self.client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
    async def generate_text(self, 
                          prompt: str, 
                          model: str = "gpt-3.5-turbo",
                          **kwargs) -> Dict[str, Any]:
        """生成文本"""
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            
            return {
                'text': response.choices[0].message.content,
                'usage': response.usage,
                'model': response.model
            }
        except Exception as e:
            logger.error(f"OpenAI API调用失败: {e}")
            return {'error': str(e)}
    
    async def generate_stream(self, 
                            prompt: str, 
                            model: str = "gpt-3.5-turbo",
                            **kwargs):
        """流式生成文本"""
        try:
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model=model,
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                **kwargs
            )
            
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"OpenAI流式API调用失败: {e}")
            yield f"错误: {str(e)}"
    
    async def embeddings(self, text: str, model: str = "text-embedding-ada-002"):
        """生成文本嵌入"""
        try:
            response = await asyncio.to_thread(
                self.client.embeddings.create,
                model=model,
                input=text
            )
            
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"OpenAI嵌入API调用失败: {e}")
            return None
```

#### 3.1.2 配置管理
```python
class OpenAIConfig:
    def __init__(self):
        self.configs = {
            'gpt-3.5-turbo': {
                'max_tokens': 4096,
                'temperature': 0.7,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'cost_per_1k_tokens': 0.002
            },
            'gpt-4': {
                'max_tokens': 8192,
                'temperature': 0.7,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'cost_per_1k_tokens': 0.03
            },
            'gpt-4-turbo': {
                'max_tokens': 128000,
                'temperature': 0.7,
                'top_p': 1.0,
                'frequency_penalty': 0.0,
                'presence_penalty': 0.0,
                'cost_per_1k_tokens': 0.01
            }
        }
    
    def get_config(self, model: str) -> Dict[str, Any]:
        """获取模型配置"""
        return self.configs.get(model, self.configs['gpt-3.5-turbo'])
    
    def calculate_cost(self, model: str, tokens_used: int) -> float:
        """计算成本"""
        config = self.get_config(model)
        cost_per_1k = config['cost_per_1k_tokens']
        return (tokens_used / 1000) * cost_per_1k
```

### 3.2 百度文心一言集成

#### 3.2.1 API客户端
```python
import requests
import json
import time
import hashlib
import hmac
import base64
from urllib.parse import urlencode

class BaiduWenxinClient:
    def __init__(self, api_key: str, secret_key: str):
        self.api_key = api_key
        self.secret_key = secret_key
        self.access_token = None
        self.token_expires = 0
        
    def _get_access_token(self):
        """获取访问令牌"""
        if self.access_token and time.time() < self.token_expires:
            return self.access_token
        
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {
            "grant_type": "client_credentials",
            "client_id": self.api_key,
            "client_secret": self.secret_key
        }
        
        response = requests.post(url, params=params)
        result = response.json()
        
        if 'access_token' in result:
            self.access_token = result['access_token']
            self.token_expires = time.time() + result['expires_in'] - 60  # 提前60秒过期
            return self.access_token
        else:
            raise Exception(f"获取访问令牌失败: {result}")
    
    async def generate_text(self, 
                          prompt: str, 
                          model: str = "ernie-bot",
                          **kwargs) -> Dict[str, Any]:
        """生成文本"""
        try:
            access_token = self._get_access_token()
            
            url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{model}"
            headers = {
                "Content-Type": "application/json"
            }
            
            data = {
                "messages": [{"role": "user", "content": prompt}],
                **kwargs
            }
            
            response = requests.post(
                f"{url}?access_token={access_token}",
                headers=headers,
                json=data
            )
            
            result = response.json()
            
            if 'result' in result:
                return {
                    'text': result['result'],
                    'usage': result.get('usage', {}),
                    'model': model
                }
            else:
                return {'error': result.get('error_msg', '未知错误')}
                
        except Exception as e:
            logger.error(f"百度文心一言API调用失败: {e}")
            return {'error': str(e)}
    
    async def generate_stream(self, 
                            prompt: str, 
                            model: str = "ernie-bot",
                            **kwargs):
        """流式生成文本"""
        try:
            access_token = self._get_access_token()
            
            url = f"https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/{model}"
            headers = {
                "Content-Type": "application/json"
            }
            
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "stream": True,
                **kwargs
            }
            
            response = requests.post(
                f"{url}?access_token={access_token}",
                headers=headers,
                json=data,
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data = line[6:]
                        if data != '[DONE]':
                            try:
                                chunk = json.loads(data)
                                if 'result' in chunk:
                                    yield chunk['result']
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            logger.error(f"百度文心一言流式API调用失败: {e}")
            yield f"错误: {str(e)}"
```

### 3.3 阿里通义千问集成

#### 3.3.1 API客户端
```python
import dashscope
from dashscope import Generation

class AlibabaTongyiClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        dashscope.api_key = api_key
        
    async def generate_text(self, 
                          prompt: str, 
                          model: str = "qwen-turbo",
                          **kwargs) -> Dict[str, Any]:
        """生成文本"""
        try:
            response = await asyncio.to_thread(
                Generation.call,
                model=model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            
            if response.status_code == 200:
                return {
                    'text': response.output.text,
                    'usage': response.usage,
                    'model': model
                }
            else:
                return {'error': response.message}
                
        except Exception as e:
            logger.error(f"阿里通义千问API调用失败: {e}")
            return {'error': str(e)}
    
    async def generate_stream(self, 
                            prompt: str, 
                            model: str = "qwen-turbo",
                            **kwargs):
        """流式生成文本"""
        try:
            responses = await asyncio.to_thread(
                Generation.call,
                model=model,
                messages=[{"role": "user", "content": prompt}],
                stream=True,
                **kwargs
            )
            
            for response in responses:
                if response.status_code == 200:
                    if response.output.text:
                        yield response.output.text
                else:
                    yield f"错误: {response.message}"
                    
        except Exception as e:
            logger.error(f"阿里通义千问流式API调用失败: {e}")
            yield f"错误: {str(e)}"
```

## 4. 统一模型接口

### 4.1 模型抽象层
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, AsyncGenerator

class BaseModel(ABC):
    """模型基类"""
    
    @abstractmethod
    async def generate_text(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """生成文本"""
        pass
    
    @abstractmethod
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """流式生成文本"""
        pass
    
    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """获取模型信息"""
        pass

class LocalModel(BaseModel):
    """本地模型"""
    
    def __init__(self, model_name: str, task_type: str, model_loader: LocalModelLoader):
        self.model_name = model_name
        self.task_type = task_type
        self.model_loader = model_loader
        self.inference = LocalModelInference(model_loader)
    
    async def generate_text(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """生成文本"""
        try:
            text = self.inference.generate_text(self.model_name, prompt, **kwargs)
            return {
                'text': text,
                'model': self.model_name,
                'type': 'local'
            }
        except Exception as e:
            return {'error': str(e)}
    
    async def generate_stream(self, prompt: str, **kwargs):
        """流式生成文本"""
        try:
            text = await self.generate_text(prompt, **kwargs)
            if 'error' not in text:
                yield text['text']
            else:
                yield f"错误: {text['error']}"
        except Exception as e:
            yield f"错误: {str(e)}"
    
    def get_model_info(self) -> Dict[str, Any]:
        """获取模型信息"""
        return {
            'name': self.model_name,
            'type': 'local',
            'task_type': self.task_type
        }

class CloudModel(BaseModel):
    """云端模型"""
    
    def __init__(self, client, model_name: str, model_type: str):
        self.client = client
        self.model_name = model_name
        self.model_type = model_type
    
    async def generate_text(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """生成文本"""
        return await self.client.generate_text(prompt, self.model_name, **kwargs)
    
    async def generate_stream(self, prompt: str, **kwargs):
        """流式生成文本"""
        async for chunk in self.client.generate_stream(prompt, self.model_name, **kwargs):
            yield chunk
    
    def get_model_info(self) -> Dict[str, Any]:
        """获取模型信息"""
        return {
            'name': self.model_name,
            'type': 'cloud',
            'provider': self.model_type
        }
```

### 4.2 模型管理器
```python
class ModelManager:
    """模型管理器"""
    
    def __init__(self):
        self.models = {}
        self.local_loader = LocalModelLoader()
        self.default_model = None
        
    def register_model(self, model_id: str, model: BaseModel):
        """注册模型"""
        self.models[model_id] = model
        if not self.default_model:
            self.default_model = model_id
    
    def get_model(self, model_id: str = None) -> BaseModel:
        """获取模型"""
        model_id = model_id or self.default_model
        if model_id not in self.models:
            raise ValueError(f"模型不存在: {model_id}")
        return self.models[model_id]
    
    def list_models(self) -> List[Dict[str, Any]]:
        """列出所有模型"""
        return [
            {
                'id': model_id,
                **model.get_model_info()
            }
            for model_id, model in self.models.items()
        ]
    
    async def generate_text(self, prompt: str, model_id: str = None, **kwargs) -> Dict[str, Any]:
        """生成文本"""
        model = self.get_model(model_id)
        return await model.generate_text(prompt, **kwargs)
    
    async def generate_stream(self, prompt: str, model_id: str = None, **kwargs):
        """流式生成文本"""
        model = self.get_model(model_id)
        async for chunk in model.generate_stream(prompt, **kwargs):
            yield chunk
    
    def switch_model(self, model_id: str):
        """切换默认模型"""
        if model_id not in self.models:
            raise ValueError(f"模型不存在: {model_id}")
        self.default_model = model_id
```

### 4.3 模型工厂
```python
class ModelFactory:
    """模型工厂"""
    
    def __init__(self):
        self.manager = ModelManager()
        
    def create_local_model(self, model_name: str, task_type: str) -> str:
        """创建本地模型"""
        model_id = f"local_{model_name}_{task_type}"
        model = LocalModel(model_name, task_type, LocalModelLoader())
        self.manager.register_model(model_id, model)
        return model_id
    
    def create_openai_model(self, api_key: str, model_name: str = "gpt-3.5-turbo") -> str:
        """创建OpenAI模型"""
        model_id = f"openai_{model_name}"
        client = OpenAIClient(api_key)
        model = CloudModel(client, model_name, "openai")
        self.manager.register_model(model_id, model)
        return model_id
    
    def create_baidu_model(self, api_key: str, secret_key: str, model_name: str = "ernie-bot") -> str:
        """创建百度模型"""
        model_id = f"baidu_{model_name}"
        client = BaiduWenxinClient(api_key, secret_key)
        model = CloudModel(client, model_name, "baidu")
        self.manager.register_model(model_id, model)
        return model_id
    
    def create_alibaba_model(self, api_key: str, model_name: str = "qwen-turbo") -> str:
        """创建阿里模型"""
        model_id = f"alibaba_{model_name}"
        client = AlibabaTongyiClient(api_key)
        model = CloudModel(client, model_name, "alibaba")
        self.manager.register_model(model_id, model)
        return model_id
    
    def get_manager(self) -> ModelManager:
        """获取模型管理器"""
        return self.manager
```

## 5. 性能优化

### 5.1 模型缓存
```python
import asyncio
from functools import lru_cache
import time

class ModelCache:
    def __init__(self, max_size: int = 10):
        self.max_size = max_size
        self.cache = {}
        self.access_times = {}
        
    def get(self, key: str):
        """获取缓存"""
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any):
        """设置缓存"""
        if len(self.cache) >= self.max_size:
            # 移除最久未访问的项
            oldest_key = min(self.access_times, key=self.access_times.get)
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = value
        self.access_times[key] = time.time()
    
    def clear(self):
        """清空缓存"""
        self.cache.clear()
        self.access_times.clear()
```

### 5.2 并发控制
```python
import asyncio
from asyncio import Semaphore

class ModelConcurrencyController:
    def __init__(self, max_concurrent: int = 5):
        self.semaphore = Semaphore(max_concurrent)
        
    async def execute_with_limit(self, coro):
        """限制并发执行"""
        async with self.semaphore:
            return await coro
```

### 5.3 负载均衡
```python
import random
from typing import List

class ModelLoadBalancer:
    def __init__(self, models: List[str]):
        self.models = models
        self.weights = {model: 1.0 for model in models}
        
    def select_model(self) -> str:
        """选择模型"""
        # 简单的轮询选择
        return random.choice(self.models)
    
    def update_weight(self, model: str, weight: float):
        """更新权重"""
        if model in self.weights:
            self.weights[model] = weight
    
    def weighted_select(self) -> str:
        """加权选择"""
        total_weight = sum(self.weights.values())
        rand = random.uniform(0, total_weight)
        
        current_weight = 0
        for model, weight in self.weights.items():
            current_weight += weight
            if rand <= current_weight:
                return model
        
        return self.models[0]  # 默认返回第一个
```

## 6. 监控和日志

### 6.1 性能监控
```python
import time
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class ModelMetrics:
    model_id: str
    request_count: int = 0
    success_count: int = 0
    error_count: int = 0
    total_tokens: int = 0
    total_cost: float = 0.0
    avg_response_time: float = 0.0
    last_request_time: float = 0.0

class ModelMonitor:
    def __init__(self):
        self.metrics = {}
        
    def record_request(self, model_id: str, start_time: float, 
                      success: bool, tokens: int = 0, cost: float = 0.0):
        """记录请求"""
        if model_id not in self.metrics:
            self.metrics[model_id] = ModelMetrics(model_id)
        
        metrics = self.metrics[model_id]
        metrics.request_count += 1
        metrics.last_request_time = time.time()
        
        if success:
            metrics.success_count += 1
        else:
            metrics.error_count += 1
        
        metrics.total_tokens += tokens
        metrics.total_cost += cost
        
        # 更新平均响应时间
        response_time = time.time() - start_time
        if metrics.request_count == 1:
            metrics.avg_response_time = response_time
        else:
            metrics.avg_response_time = (
                (metrics.avg_response_time * (metrics.request_count - 1) + response_time) 
                / metrics.request_count
            )
    
    def get_metrics(self, model_id: str = None) -> Dict[str, Any]:
        """获取指标"""
        if model_id:
            return self.metrics.get(model_id, ModelMetrics(model_id))
        else:
            return {mid: metrics for mid, metrics in self.metrics.items()}
    
    def get_summary(self) -> Dict[str, Any]:
        """获取汇总信息"""
        total_requests = sum(m.request_count for m in self.metrics.values())
        total_success = sum(m.success_count for m in self.metrics.values())
        total_errors = sum(m.error_count for m in self.metrics.values())
        total_tokens = sum(m.total_tokens for m in self.metrics.values())
        total_cost = sum(m.total_cost for m in self.metrics.values())
        
        return {
            'total_requests': total_requests,
            'total_success': total_success,
            'total_errors': total_errors,
            'success_rate': total_success / total_requests if total_requests > 0 else 0,
            'total_tokens': total_tokens,
            'total_cost': total_cost,
            'models': len(self.metrics)
        }
```

### 6.2 错误处理
```python
class ModelErrorHandler:
    def __init__(self):
        self.error_counts = {}
        self.error_threshold = 5
        
    def handle_error(self, model_id: str, error: Exception):
        """处理错误"""
        if model_id not in self.error_counts:
            self.error_counts[model_id] = 0
        
        self.error_counts[model_id] += 1
        
        # 记录错误日志
        logger.error(f"模型 {model_id} 发生错误: {error}")
        
        # 检查是否超过错误阈值
        if self.error_counts[model_id] >= self.error_threshold:
            logger.warning(f"模型 {model_id} 错误次数过多，建议检查")
    
    def reset_error_count(self, model_id: str):
        """重置错误计数"""
        self.error_counts[model_id] = 0
    
    def get_error_count(self, model_id: str) -> int:
        """获取错误计数"""
        return self.error_counts.get(model_id, 0)
```

## 7. 配置管理

### 7.1 配置文件
```yaml
# models_config.yaml
models:
  local:
    text_generation:
      - name: "gpt2"
        path: "gpt2"
        max_length: 100
        temperature: 0.7
      - name: "chinese-gpt2"
        path: "uer/gpt2-chinese-cluecorpussmall"
        max_length: 100
        temperature: 0.7
    text_classification:
      - name: "bert-base-chinese"
        path: "bert-base-chinese"
    question_answering:
      - name: "bert-base-chinese"
        path: "bert-base-chinese"
  
  cloud:
    openai:
      api_key: "${OPENAI_API_KEY}"
      models:
        - name: "gpt-3.5-turbo"
          max_tokens: 4096
          temperature: 0.7
        - name: "gpt-4"
          max_tokens: 8192
          temperature: 0.7
    baidu:
      api_key: "${BAIDU_API_KEY}"
      secret_key: "${BAIDU_SECRET_KEY}"
      models:
        - name: "ernie-bot"
          max_tokens: 2048
          temperature: 0.7
    alibaba:
      api_key: "${ALIBABA_API_KEY}"
      models:
        - name: "qwen-turbo"
          max_tokens: 4096
          temperature: 0.7

default_model: "openai_gpt-3.5-turbo"
max_concurrent_requests: 5
cache_size: 10
error_threshold: 5
```

### 7.2 配置加载器
```python
import yaml
import os
from typing import Dict, Any

class ConfigLoader:
    def __init__(self, config_path: str = "models_config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """加载配置"""
        if not os.path.exists(self.config_path):
            return self._get_default_config()
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # 替换环境变量
        config = self._replace_env_vars(config)
        return config
    
    def _replace_env_vars(self, config: Any) -> Any:
        """替换环境变量"""
        if isinstance(config, str) and config.startswith("${") and config.endswith("}"):
            env_var = config[2:-1]
            return os.getenv(env_var, "")
        elif isinstance(config, dict):
            return {k: self._replace_env_vars(v) for k, v in config.items()}
        elif isinstance(config, list):
            return [self._replace_env_vars(item) for item in config]
        else:
            return config
    
    def _get_default_config(self) -> Dict[str, Any]:
        """获取默认配置"""
        return {
            'models': {
                'local': {},
                'cloud': {}
            },
            'default_model': 'local_gpt2_text_generation',
            'max_concurrent_requests': 5,
            'cache_size': 10,
            'error_threshold': 5
        }
    
    def get_model_config(self, model_type: str, provider: str = None) -> Dict[str, Any]:
        """获取模型配置"""
        if provider:
            return self.config['models'].get(model_type, {}).get(provider, {})
        else:
            return self.config['models'].get(model_type, {})
    
    def get_default_model(self) -> str:
        """获取默认模型"""
        return self.config.get('default_model', 'local_gpt2_text_generation')
    
    def get_system_config(self) -> Dict[str, Any]:
        """获取系统配置"""
        return {
            'max_concurrent_requests': self.config.get('max_concurrent_requests', 5),
            'cache_size': self.config.get('cache_size', 10),
            'error_threshold': self.config.get('error_threshold', 5)
        }
```
