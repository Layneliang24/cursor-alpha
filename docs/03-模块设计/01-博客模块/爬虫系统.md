# 博客模块 - 爬虫系统设计（概要）

## 1. 目标
- 配置化抓取技术站点最新文章；支持去重、质量评分、定时调度、失败重试与回滚。

## 2. 架构
- 配置层：站点、入口、选择器/规则、频率、优先级、去重键。
- 调度层：Celery Beat 产出任务，Worker 消费。
- 抓取层：requests/bs4（必要时 Selenium）。
- 解析层：站点适配器（Adapter）。
- 入库层：质量评分、草稿入库、日志与统计。

## 3. 关键模型（摘要）
- crawler_configs(id, name, url, rules,json, frequency, priority, status)
- crawler_jobs(id, config_id, status, started_at, finished_at, stats,json)
- crawler_logs(id, job_id, level, message, created_at)

## 4. 去重与评分
- 去重键：hash(title + source_url) 或 DOM 指纹。
- 评分因子：标题关键词、内容长度、代码块占比、源站权重。

## 5. 任务流程
1) Beat 根据 frequency 生成任务；  
2) Worker 抓取→解析→去重/评分→入库草稿；  
3) 记录日志/统计，失败指数退避重试。

## 6. 伪代码
```python
class CrawlerService:
    def run(self, config_id: int):
        cfg = get_config(config_id)
        html = http_get(cfg.url)
        items = adapter(cfg).parse_list(html)
        for it in items:
            if is_duplicate(it):
                continue
            it['quality'] = quality_score(it)
            save_as_draft(it)
```

## 7. 运维（2GB）
- worker=1、prefetch=1；连接/读超时；监控任务成功率/平均用时/失败原因 TopN。
