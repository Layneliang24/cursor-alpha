# 博客模块 - 爬虫系统设计

## 1. 目标
- 配置化抓取技术站点最新文章；支持去重、质量评分、定时调度、失败重试与回滚。
- 自动获取高质量技术内容，丰富博客内容库。

## 2. 数据源配置

### 2.1 推荐技术站点（免费数据源）

#### 中文技术站点
```json
{
  "name": "掘金",
  "url": "https://juejin.cn",
  "type": "tech_blog",
  "language": "zh",
  "rules": {
    "list_selector": ".entry-list .entry",
    "title_selector": ".title",
    "link_selector": "a",
    "summary_selector": ".abstract",
    "author_selector": ".user-name"
  }
}
```

```json
{
  "name": "CSDN",
  "url": "https://www.csdn.net",
  "type": "tech_blog", 
  "language": "zh",
  "rules": {
    "list_selector": ".article-item-box",
    "title_selector": ".content-title",
    "link_selector": "a",
    "summary_selector": ".content-desc",
    "author_selector": ".user-name"
  }
}
```

```json
{
  "name": "InfoQ中文",
  "url": "https://www.infoq.cn",
  "type": "tech_news",
  "language": "zh",
  "rules": {
    "list_selector": ".article-item",
    "title_selector": ".article-title",
    "link_selector": "a",
    "summary_selector": ".article-summary",
    "author_selector": ".author-name"
  }
}
```

#### 英文技术站点
```json
{
  "name": "Medium",
  "url": "https://medium.com",
  "type": "tech_blog",
  "language": "en",
  "rules": {
    "list_selector": "article",
    "title_selector": "h2",
    "link_selector": "a[href*='/p/']",
    "summary_selector": "p",
    "author_selector": ".author-name"
  }
}
```

```json
{
  "name": "Dev.to",
  "url": "https://dev.to",
  "type": "tech_blog",
  "language": "en",
  "rules": {
    "list_selector": ".crayons-story",
    "title_selector": ".crayons-story__title",
    "link_selector": "a",
    "summary_selector": ".crayons-story__snippet",
    "author_selector": ".crayons-story__author"
  }
}
```

```json
{
  "name": "Hacker News",
  "url": "https://news.ycombinator.com",
  "type": "tech_news",
  "language": "en",
  "rules": {
    "list_selector": ".athing",
    "title_selector": ".titleline a",
    "link_selector": "a",
    "summary_selector": ".subtext",
    "author_selector": ".hnuser"
  }
}
```

### 2.2 技术文档站点
```json
{
  "name": "MDN Web Docs",
  "url": "https://developer.mozilla.org",
  "type": "documentation",
  "language": "en",
  "rules": {
    "list_selector": ".card-grid .card",
    "title_selector": ".card-title",
    "link_selector": "a",
    "summary_selector": ".card-description"
  }
}
```

```json
{
  "name": "GitHub Trending",
  "url": "https://github.com/trending",
  "type": "repository",
  "language": "en",
  "rules": {
    "list_selector": "article.Box-row",
    "title_selector": "h2 a",
    "link_selector": "h2 a",
    "summary_selector": "p",
    "language_selector": "[itemprop='programmingLanguage']"
  }
}
```

### 2.3 RSS源配置
```json
{
  "name": "阮一峰的网络日志",
  "url": "http://www.ruanyifeng.com/blog/atom.xml",
  "type": "rss",
  "language": "zh",
  "category": "tech_blog"
}
```

```json
{
  "name": "Python官方博客",
  "url": "https://blog.python.org/feed/",
  "type": "rss", 
  "language": "en",
  "category": "tech_blog"
}
```

## 3. 架构设计

### 3.1 系统架构
- **配置层**: 站点、入口、选择器/规则、频率、优先级、去重键
- **调度层**: Celery Beat 产出任务，Worker 消费
- **抓取层**: requests/bs4（必要时 Selenium）
- **解析层**: 站点适配器（Adapter）
- **入库层**: 质量评分、草稿入库、日志与统计

### 3.2 数据模型
```python
# backend/apps/articles/models.py
class CrawlerConfig(models.Model):
    name = models.CharField(max_length=100, verbose_name="配置名称")
    url = models.URLField(verbose_name="目标URL")
    site_type = models.CharField(max_length=50, choices=[
        ('tech_blog', '技术博客'),
        ('tech_news', '技术新闻'),
        ('documentation', '技术文档'),
        ('repository', '代码仓库'),
        ('rss', 'RSS源')
    ])
    language = models.CharField(max_length=10, choices=[
        ('zh', '中文'),
        ('en', '英文')
    ])
    rules = models.JSONField(verbose_name="解析规则")
    frequency = models.CharField(max_length=20, choices=[
        ('hourly', '每小时'),
        ('daily', '每天'),
        ('weekly', '每周'),
        ('manual', '手动')
    ])
    priority = models.IntegerField(default=5, verbose_name="优先级")
    is_active = models.BooleanField(default=True, verbose_name="是否启用")
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)

class CrawlerJob(models.Model):
    config = models.ForeignKey(CrawlerConfig, on_delete=models.CASCADE)
    status = models.CharField(max_length=20, choices=[
        ('pending', '等待中'),
        ('running', '执行中'),
        ('completed', '已完成'),
        ('failed', '失败')
    ])
    started_at = models.DateTimeField(null=True)
    finished_at = models.DateTimeField(null=True)
    stats = models.JSONField(default=dict, verbose_name="统计信息")
    error_message = models.TextField(blank=True, verbose_name="错误信息")

class CrawlerLog(models.Model):
    job = models.ForeignKey(CrawlerJob, on_delete=models.CASCADE)
    level = models.CharField(max_length=10, choices=[
        ('DEBUG', '调试'),
        ('INFO', '信息'),
        ('WARNING', '警告'),
        ('ERROR', '错误')
    ])
    message = models.TextField()
    created_at = models.DateTimeField(auto_now_add=True)
```

## 4. 核心功能

### 4.1 去重与评分
- **去重键**: hash(title + source_url) 或 DOM 指纹
- **评分因子**: 
  - 标题关键词匹配度 (30%)
  - 内容长度 (20%)
  - 代码块占比 (15%)
  - 源站权重 (20%)
  - 发布时间新鲜度 (15%)

### 4.2 质量评分算法
```python
def calculate_quality_score(article_data):
    score = 0
    
    # 标题关键词匹配 (30分)
    title_keywords = ['python', 'django', 'vue', 'docker', 'kubernetes']
    title_lower = article_data['title'].lower()
    keyword_matches = sum(1 for kw in title_keywords if kw in title_lower)
    score += (keyword_matches / len(title_keywords)) * 30
    
    # 内容长度 (20分)
    content_length = len(article_data['content'])
    if content_length > 1000:
        score += 20
    elif content_length > 500:
        score += 15
    elif content_length > 200:
        score += 10
    
    # 代码块占比 (15分)
    code_blocks = article_data['content'].count('```')
    if code_blocks > 2:
        score += 15
    elif code_blocks > 0:
        score += 10
    
    # 源站权重 (20分)
    site_weights = {
        'juejin.cn': 20,
        'medium.com': 18,
        'dev.to': 16,
        'csdn.net': 12
    }
    score += site_weights.get(article_data['source'], 10)
    
    # 发布时间新鲜度 (15分)
    days_old = (datetime.now() - article_data['published_date']).days
    if days_old <= 1:
        score += 15
    elif days_old <= 7:
        score += 10
    elif days_old <= 30:
        score += 5
    
    return min(score, 100)
```

## 5. 任务流程
1. **Beat 根据 frequency 生成任务**
2. **Worker 抓取→解析→去重/评分→入库草稿**
3. **记录日志/统计，失败指数退避重试**

### 5.1 任务执行流程
```python
class CrawlerService:
    def run(self, config_id: int):
        try:
            # 1. 获取配置
            cfg = CrawlerConfig.objects.get(id=config_id)
            
            # 2. 创建任务记录
            job = CrawlerJob.objects.create(
                config=cfg,
                status='running',
                started_at=timezone.now()
            )
            
            # 3. 抓取内容
            html = self.http_get(cfg.url)
            
            # 4. 解析内容
            items = self.parse_content(html, cfg.rules)
            
            # 5. 处理每个项目
            for item in items:
                if self.is_duplicate(item):
                    continue
                    
                item['quality_score'] = self.calculate_quality_score(item)
                
                # 只保存高质量内容
                if item['quality_score'] >= 60:
                    self.save_as_draft(item, cfg)
            
            # 6. 更新任务状态
            job.status = 'completed'
            job.finished_at = timezone.now()
            job.stats = {
                'total_items': len(items),
                'saved_items': len([i for i in items if i['quality_score'] >= 60]),
                'avg_quality': sum(i['quality_score'] for i in items) / len(items) if items else 0
            }
            job.save()
            
        except Exception as e:
            job.status = 'failed'
            job.error_message = str(e)
            job.finished_at = timezone.now()
            job.save()
            
            # 记录错误日志
            CrawlerLog.objects.create(
                job=job,
                level='ERROR',
                message=f"爬虫任务失败: {str(e)}"
            )
```

## 6. 运维配置（2GB服务器优化）

### 6.1 Celery配置
```python
# backend/alpha/celery.py
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'

# 2GB服务器优化配置
CELERY_WORKER_CONCURRENCY = 1  # 单进程
CELERY_WORKER_PREFETCH_MULTIPLIER = 1  # 减少内存占用
CELERY_TASK_ACKS_LATE = True  # 任务完成后再确认
CELERY_TASK_REJECT_ON_WORKER_LOST = True  # 工作进程丢失时拒绝任务

# 任务超时设置
CELERY_TASK_SOFT_TIME_LIMIT = 300  # 5分钟软超时
CELERY_TASK_TIME_LIMIT = 600  # 10分钟硬超时
```

### 6.2 监控指标
- 任务成功率
- 平均执行时间
- 失败原因TopN
- 内存使用情况
- 网络请求成功率

### 6.3 错误处理
```python
# 指数退避重试
@shared_task(bind=True, max_retries=3)
def crawl_task(self, config_id):
    try:
        CrawlerService().run(config_id)
    except Exception as exc:
        # 指数退避重试
        countdown = 2 ** self.request.retries
        self.retry(countdown=countdown, exc=exc)
```

## 7. 数据源管理

### 7.1 数据源分类
- **技术博客**: 掘金、CSDN、Medium、Dev.to
- **技术新闻**: InfoQ、Hacker News
- **技术文档**: MDN、官方文档
- **代码仓库**: GitHub Trending
- **RSS源**: 个人博客、官方博客

### 7.2 数据源权重
```python
SITE_WEIGHTS = {
    # 高质量技术博客
    'juejin.cn': 20,
    'medium.com': 18,
    'dev.to': 16,
    'infoq.cn': 15,
    
    # 技术新闻
    'news.ycombinator.com': 14,
    
    # 技术文档
    'developer.mozilla.org': 12,
    
    # 其他站点
    'csdn.net': 10,
    'default': 8
}
```

### 7.3 内容过滤规则
```python
CONTENT_FILTERS = {
    'min_length': 200,  # 最小内容长度
    'max_length': 50000,  # 最大内容长度
    'required_keywords': ['python', 'django', 'vue', 'docker'],  # 必需关键词
    'blocked_keywords': ['广告', '推广', 'spam'],  # 屏蔽关键词
    'min_quality_score': 60  # 最低质量分数
}
```

